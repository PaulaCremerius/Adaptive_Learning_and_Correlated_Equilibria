{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the algorithm described in Hart and Mas-Colell (2000)\n",
    "\n",
    "# This script implements the Hart-Mas-Colell algorithm for finding correlated\n",
    "# equilibria in many-player many-action games via the players following a simple\n",
    "# adaptive procedure.\n",
    " \n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "%matplotlib notebook\n",
    "import pickle \n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import scipy.io\n",
    "\n",
    "# players are 0, 1, 2, ..., n-1\n",
    " \n",
    "# Game 'matrix': each player i chooses index i and gets payoff equal to element\n",
    "# i of the resulting list\n",
    " \n",
    "# so, payoff to player i is\n",
    "# matrix[player_0_action][player_1_action]...[player_n-1_action][i]\n",
    " \n",
    "# For the code to work, all entries of the matrix must be non-negative numbers.\n",
    " \n",
    "matching_pennies = np.array([[[0,1], [1,0]],[[1,0], [0,1]]]) \n",
    "\n",
    "    \n",
    "bach_stravinsky = np.array([[[2,1], [0,0]],\n",
    "                            [[0,0], [1,2]]])\n",
    " \n",
    "prisoners_dilemma = np.array([[[2,2], [0,4]],\n",
    "                              [[4,0], [1,1]]])\n",
    " \n",
    "chicken = np.array([[[6,6], [2,7]],\n",
    "                    [[7,2], [0,0]]]) \n",
    "\n",
    "games_dict = {}\n",
    "games_dict['matching_pennies'] = matching_pennies\n",
    "games_dict['prisoners_dilemma'] = prisoners_dilemma\n",
    "games_dict['chicken'] = chicken\n",
    "games_dict['bach_stravinsky'] = bach_stravinsky\n",
    "\n",
    "rules_dict = {}\n",
    "rules_dict[\"1\"] = \"Base Rule\"\n",
    "rules_dict[\"1'\"] = \"Base Rule without Inertia\"\n",
    "rules_dict[\"2\"] = \"Proxy Regrets\"\n",
    "rules_dict[\"2'\"] = \"Proxy Regrets without Inertia\"\n",
    "rules_dict[\"2b\"] = \"BAN Proxy Regrets\"\n",
    "rules_dict[\"2c\"] = \"LAN Proxy Regrets\"\n",
    "rules_dict[\"2d\"] = \"Average Regret\"\n",
    "\n",
    "class GameData:\n",
    "\n",
    "    def __init__(self, game_name, version, num_runs, num_steps, predetermined_steps_version = None):\n",
    "        self.name = game_name\n",
    "        self.version = version\n",
    "        self.num_runs = num_runs\n",
    "        self.num_steps = num_steps\n",
    "        self.game_matrix = games_dict.get(game_name)\n",
    "        self.num_players = np.ndim(self.game_matrix) - 1\n",
    "        self.config = '_' + str(self.num_runs) + 'runs_' + str(self.num_steps) + 'steps'\n",
    "        self.largest_payoff = np.amax(np.abs(np.ndarray.flatten(self.game_matrix)))\n",
    "        for i in np.ndarray.flatten(self.game_matrix):\n",
    "            assert i >= 0, \"all payoffs should be non-negative\"\n",
    "        self.num_actions_ = [np.shape(self.game_matrix)[player] for player in range(self.num_players)]\n",
    "        if predetermined_steps_version == None:\n",
    "            self.prob_predetermined_steps = [[[1/self.num_actions_[player] for i in range(self.num_actions_[player])] for player in range(self.num_players)]] \n",
    "        else:\n",
    "            self.prob_predetermined_steps = (games_first_steps_dict[self.name])[predetermined_steps_version]\n",
    "            print(self.prob_predetermined_steps[0])\n",
    "        self.num_predetermined_steps = np.shape(self.prob_predetermined_steps)[0]\n",
    "        self.faktor_constante =  1.1; # when payoffs >0:  >*1 \n",
    "        self.delta = 0.55\n",
    "        self.delta_power =  0.2 # for reasons and options of choosing delta see Paper\n",
    "        self.denom_ratio_if_divideByZero = 1\n",
    "        \n",
    "    def save_history(self, history):\n",
    "        self.history = history\n",
    "        self.history_creation_time = str(datetime.datetime.now())\n",
    "        \n",
    "    def save_emp_dists(self, emp_dists): \n",
    "        self.emp_dists = np.array(emp_dists)\n",
    "        \n",
    "    def save_ce_distances(self, ce_distances):\n",
    "        self.ce_distances = np.array(ce_distances)\n",
    "        \n",
    "    def save_eukl_distances(self, eukl_distances):\n",
    "        self.eukl_distances = np.array(eukl_distances)\n",
    "        \n",
    "    def save_probs(self, probs, payoff_diffs):\n",
    "        self.probs = probs\n",
    "        self.payoff_diffs = payoff_diffs\n",
    "        \n",
    "    def delete_history_data(self):\n",
    "        self.history = []\n",
    "        \n",
    "    def delete_emp_dist_data(self):\n",
    "        self.emp_dists = []\n",
    "        \n",
    "    def get_predet_save_text(self):\n",
    "        if self.num_predetermined_steps >1:\n",
    "            return '_'+str(self.num_predetermined_steps)+'steps_fixed'\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(game):\n",
    "    start_time_total = time.time()\n",
    "    history_ = []\n",
    "    probs_ = []\n",
    "    payoff_diffs_ = []\n",
    "    for j in range(game.num_runs):\n",
    "        [game_history, probs, payoff_diffs] = start_run(game)\n",
    "        history_.append(game_history)\n",
    "        probs_.append(probs)\n",
    "        payoff_diffs_.append(payoff_diffs)\n",
    "        if (j*1000/game.num_runs)% 1 == 0:\n",
    "            update_progress(game, j/ game.num_runs,' simulating game')\n",
    "    update_progress(game, 1,' simulating game')\n",
    "    game.save_history(history_) \n",
    "    print('Elapsed: %.2f seconds' % (time.time() - start_time_total))\n",
    "    time.sleep(1)\n",
    "    add_emp_dists(game)\n",
    "    compute_ce_distances(game)\n",
    "    if(text_output_mode == 1 or text_output_mode == 2 ):\n",
    "        game.save_probs(np.array(probs_), np.array(payoff_diffs_)) # conains only data from last run\n",
    "    add_entropy_values(game)\n",
    "    print('Total time elapsed: %.2f seconds' % (time.time() - start_time_total))\n",
    "    \n",
    "\n",
    "# Play num_steps of the game\n",
    "def start_run(game):\n",
    "    [all_payoff_diffs, last_prob_array, sum_payoff_diffs, denom2] = initialize_game(game)\n",
    "    # first round\n",
    "    [last_actions, last_payoffs, game_history, all_probs] = play_first_round(game)\n",
    "    # next rounds\n",
    "    for t in range(1, game.num_steps):\n",
    "        next_actions = []\n",
    "        for player in range(game.num_players):\n",
    "            old_action = last_actions[player]\n",
    "            old_payoff = last_payoffs[player]\n",
    "            # could also be calculated for all players at once, to make clear that versions >0 don't need \n",
    "            # full information, versions >0 will only use last_actions[player]\n",
    "            calculate_new_payoff_diff(game, sum_payoff_diffs, denom2, last_actions, last_prob_array[player], player, old_payoff)\n",
    "            if t < game.num_predetermined_steps and game.version != \"3\": #3: random\n",
    "                next_actions.append(np.random.choice(game.num_actions_[player], p = game.prob_predetermined_steps[t][player]))\n",
    "            else:\n",
    "                regret_array = calculate_regret(game, player, old_action, sum_payoff_diffs, denom2, t)\n",
    "                last_prob_array[player] = prob_next_action(game, player, old_action, regret_array, t) # must be saved to use in update_payoff_diffs in next iteration\n",
    "                next_actions.append(np.random.choice(game.num_actions_[player], p = last_prob_array[player]))\n",
    "        \n",
    "        # save game info\n",
    "        last_actions = tuple(next_actions)\n",
    "        last_payoffs = game.game_matrix[last_actions]\n",
    "        game_history.append(last_actions)  \n",
    "    return game_history, all_probs, all_payoff_diffs\n",
    "\n",
    "\n",
    "def initialize_game(game): \n",
    "    player_ = range(game.num_players)  \n",
    "    last_prob_array = game.prob_predetermined_steps[0].copy()\n",
    "    all_payoff_diffs = []\n",
    "    sum_payoff_diffs = [[] for player in player_]\n",
    "    if version in [\"1\",\"2\",\"2d\",\"2b\",\"2c\",\"3\"]:\n",
    "        for player in player_:\n",
    "            range_act = range(game.num_actions_[player])\n",
    "            sum_payoff_diffs[player] = [[] for old_action in range_act]\n",
    "            for old_action in range_act:\n",
    "                sum_payoff_diffs[player][old_action] = [0 for alt_action in range_act]\n",
    "        if version == \"2d\": \n",
    "            sum_payoff_diffs = np.array([sum_payoff_diffs, sum_payoff_diffs])\n",
    "            denom2 = np.array(sum_payoff_diffs.copy())\n",
    "        else:\n",
    "            denom2 = np.array([])\n",
    "    elif version in [\"1'\",\"2'\"]:\n",
    "        for player in player_:\n",
    "            sum_payoff_diffs[player] = [0 for old_action in range(game.num_actions_[player])] # Sufficient for version 5 and 6\n",
    "        denom2 = np.array([])\n",
    "    sum_payoff_diffs = np.array(sum_payoff_diffs)/1\n",
    "    return all_payoff_diffs, last_prob_array, sum_payoff_diffs, denom2\n",
    "\n",
    "\n",
    "def play_first_round(game):\n",
    "    # initial moves\n",
    "    initial_actions = []  \n",
    "    all_probs = []\n",
    "    for player in range(game.num_players):\n",
    "        initial_actions.append(np.random.choice(game.num_actions_[player], p = game.prob_predetermined_steps[0][player]))\n",
    "    last_actions = tuple(initial_actions)\n",
    "    game_history = [last_actions]\n",
    "    last_payoffs = game.game_matrix[last_actions]\n",
    "    return last_actions, last_payoffs, game_history, all_probs\n",
    " \n",
    "    \n",
    "    \n",
    "def replace_at_index(tup, index, val):\n",
    "    return tup[:index] + (val,) + tup[index + 1:]\n",
    " \n",
    "\n",
    "# sum_payoff_diffs[player, old_action, alt_action] is representing the difference in utilities players \n",
    "# would have gotten in the past by playing alt_action wherever they actually played old_action\n",
    "def calculate_new_payoff_diff(game, sum_payoff_diffs, denom2, last_actions, last_prob_array_player, player, old_payoff):\n",
    "    #[player, old_action, alt(ernative)_action]\n",
    "    old_diffs = sum_payoff_diffs.copy()\n",
    "    old_denom2 = denom2.copy()   \n",
    "    old_action = last_actions[player] # For versions >0 only own history is available not all players last actions\n",
    "    ratio_denominator = last_prob_array_player[old_action] if last_prob_array_player[old_action]> 0  else game.denom_ratio_if_divideByZero\n",
    "    \n",
    "    for other_action in range(game.num_actions_[player]):\n",
    "        if game.version == \"1'\": # Paper 1: Theorem B (4.2) no focus on last action\n",
    "            other_action_tuple = replace_at_index(last_actions, player, other_action)\n",
    "            other_payoff = game.game_matrix[other_action_tuple][player]\n",
    "            sum_payoff_diffs[player][other_action] = old_diffs[player][other_action] + other_payoff - old_payoff\n",
    "            \n",
    "        elif game.version == \"2'\": # Paper 2: 3c) (8) proxi regrets with no focus on last action\n",
    "            sum_payoff_diffs[player][other_action] = old_diffs[player][other_action] - old_payoff\n",
    "            if other_action == old_action:\n",
    "                sum_payoff_diffs[player][other_action] = old_diffs[player][other_action] + (old_payoff/ratio_denominator)\n",
    "        else:\n",
    "            if other_action != old_action: # else payoff diff is 0 trivially\n",
    "                if game.version == \"1\": # Paper 1\n",
    "                    other_action_tuple = replace_at_index(last_actions, player, other_action)\n",
    "                    other_payoff = game.game_matrix[other_action_tuple][player]\n",
    "                    sum_payoff_diffs[player][old_action][other_action] = old_diffs[player][old_action][other_action] + other_payoff - old_payoff\n",
    "\n",
    "                elif game.version == \"2\": # Paper 2: Proxy-Regrets (probRatio with positive term)\n",
    "                    sum_payoff_diffs[player][old_action][other_action] = old_diffs[player][old_action][other_action] - old_payoff  \n",
    "                    probability_ratio = last_prob_array_player[other_action]/ratio_denominator\n",
    "                    sum_payoff_diffs[player][other_action][old_action] = old_diffs[player][other_action][old_action] + probability_ratio*old_payoff\n",
    "\n",
    "                elif game.version == \"2d\": # averages\n",
    "                    # numerators: summed payoffs [0]: positive term [1]: negative term\n",
    "                    sum_payoff_diffs[0][player][other_action][old_action] = old_diffs[0][player][other_action][old_action] +old_payoff\n",
    "                    sum_payoff_diffs[1][player][old_action][other_action] = old_diffs[1][player][old_action][other_action] -old_payoff\n",
    "                    # denominators: number of entrys [0]: positive term [1]: negative term\n",
    "                    denom2[0][player][other_action][old_action] = old_denom2[0][player][other_action][old_action] + 1\n",
    "                    denom2[1][player][old_action][other_action] = old_denom2[1][player][old_action][other_action] + 1\n",
    "\n",
    "                elif game.version == \"2b\": # Paper 2: Proxy-Regrets, probabilities as part of both terms\n",
    "                    sum_payoff_diffs[player][old_action][other_action] = old_diffs[player][old_action][other_action] - old_payoff/last_prob_array_player[old_action]\n",
    "                    sum_payoff_diffs[player][other_action][old_action] = old_diffs[player][other_action][old_action] + old_payoff/last_prob_array_player[old_action]\n",
    "\n",
    "                elif game.version == \"2c\": # Paper 2: Proxy-Regrets, probRatio with negative term\n",
    "                    probability_ratio = last_prob_array_player[other_action]/ ratio_denominator\n",
    "                    sum_payoff_diffs[player][old_action][other_action] = old_diffs[player][old_action][other_action] - probability_ratio*old_payoff\n",
    "                    sum_payoff_diffs[player][other_action][old_action] = old_diffs[player][other_action][old_action] + old_payoff\n",
    "\n",
    "\n",
    "def calculate_regret(game, player, old_action, sum_payoff_diffs, denom2, hist_length):\n",
    "    if game.version == \"2d\":\n",
    "        denom = denom2 + (denom2 == 0) # set zero entries to 1\n",
    "        regret = np.sum(sum_payoff_diffs/denom, axis = 0)\n",
    "    else:\n",
    "        regret = sum_payoff_diffs/hist_length\n",
    "        \n",
    "    if game.version == \"1'\" or game.version == \"2'\":\n",
    "        result = [max(0, regret[player][alt_action]) for alt_action in range(game.num_actions_[player])]\n",
    "    else:\n",
    "        result = [max(0, regret[player][old_action][alt_action]) for alt_action in range(game.num_actions_[player])]\n",
    "    return result\n",
    "\n",
    "\n",
    "# this function computes the distribution over new actions given a previous\n",
    "# action and an array of regrets for playing the previous action instead of\n",
    "# other actions. this array has one entry for every action, and the entry for\n",
    "# the previous action must be zero.   \n",
    "def prob_next_action(game, player, last_action, regret_array, hist_length):\n",
    "    num_act = game.num_actions_[player]\n",
    "    normalisation_const = (num_act - 1) * game.largest_payoff* game.faktor_constante; # paper says > *2, when payoffs >0:  >*1 \n",
    "    delta_t = game.delta/(np.power(hist_length, game.delta_power)) # for reasons and options of choosing delta see Paper2 not used in versions 0 & 5\n",
    "        \n",
    "    if game.version == \"1\": # Paper 1\n",
    "        assert regret_array[last_action] == 0, \"your regret isn't zero when it should obviously be\"\n",
    "        probs_array = [regret / normalisation_const for regret in regret_array]\n",
    "        probs_array[last_action] = 1 - sum(probs_array)\n",
    "        \n",
    "    elif game.version in [\"2\", \"2d\", \"2b\", \"2c\"]: \n",
    "        probs_array = [[] for p in range(num_act)]\n",
    "        for entry in range(game.num_actions_[player]):\n",
    "            regret = regret_array[entry]\n",
    "            bounded_regrets = min(regret/ normalisation_const, 1/(num_act-1))\n",
    "            probs_array[entry] = (1 - delta_t)* bounded_regrets + delta_t* (1/ num_act)\n",
    "        probs_array[last_action] = 0 \n",
    "        probs_array[last_action] = 1 - sum(probs_array)\n",
    "        \n",
    "    elif game.version == \"1'\" or game.version == \"2'\":\n",
    "        regret_sum = np.sum(regret_array)\n",
    "        if regret_sum != 0:\n",
    "            if version == \"1'\": \n",
    "                probs_array = [regret / regret_sum for regret in regret_array]\n",
    "            elif version == \"2'\":\n",
    "                probs_array = [(1 - delta_t)*(regret/ regret_sum)+ delta_t* (1/ num_act)  for regret in regret_array]\n",
    "        else:\n",
    "            probs_array = [1/num_act for i in range(num_act)]\n",
    "    else:\n",
    "        probs_array = [1/num_act for i in range(num_act)]\n",
    "    return probs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empirical distributions of play for game historys of a BATCH OF HISTORYS, uses createEmpiricalDist\n",
    "def add_emp_dists(game):\n",
    "    emp_dists = []\n",
    "    start_time = time.time()\n",
    "    for i in range(game.num_runs):\n",
    "        emp_dists.append(create_empirical_dist(game, game.history[i])) #if game is handed down further game_copy = copy.copy(game)\n",
    "        if (i*100/game.num_runs)% 2 == 0:\n",
    "            update_progress(game, i/ game.num_runs, ' compute empirical distributions')\n",
    "    update_progress(game, 1, ' compute empirical distributions')\n",
    "    print('Elapsed: %.2f seconds' % (time.time() - start_time)) \n",
    "    time.sleep(1) \n",
    "    game.save_emp_dists(emp_dists) \n",
    "\n",
    "\n",
    "# Create Empirical distribution of play for game history of a SINGLE run\n",
    "def create_empirical_dist(game, single_run_history):\n",
    "    range_action_profiles = range(np.prod(game.num_actions_))\n",
    "    indices = [int(np.prod(game.num_actions_[0:i])) for i in range(game.num_players+1)]\n",
    "    dist = [[0 for l in range_action_profiles]]\n",
    "    dist[0][sum([single_run_history[0][p]*indices[p] for p in range(game.num_players)])] += 1\n",
    "    for i in range(1, game.num_steps):\n",
    "        new_dist = copy.deepcopy(dist[i-1])\n",
    "        new_dist[sum([single_run_history[i][p]*indices[p] for p in range(game.num_players)] )] +=1\n",
    "        dist.append(new_dist)\n",
    "    emp_dists = np.array([[dist[t][l]/(t+1) for l in range_action_profiles] for t in range(game.num_steps)])\n",
    "    return emp_dists\n",
    "\n",
    "\n",
    "def compute_ce_distances(game):\n",
    "    ce_distances =[]\n",
    "    start_time = time.time()\n",
    "    for i in range(game.num_runs):\n",
    "        ce_distances.append(measure_for_correlated_eq(game, game.emp_dists[i]))\n",
    "        if (i*100/game.num_runs)%2 == 0:\n",
    "            update_progress(game, i/ game.num_runs, ' compute ce-distances')\n",
    "    update_progress(game, 1, ' compute ce-distances')\n",
    "    print('Elapsed: %.2f seconds' % (time.time() - start_time))\n",
    "    time.sleep(1) \n",
    "    game.save_ce_distances(ce_distances)\n",
    "\n",
    "\n",
    "# Computes for the empirial distribution of play of ONE game run for each time step the distance of the empirical\n",
    "# distribution of play to a correlated equilibrium, the used measure is the maximum of the vector from the \n",
    "# correlated equilibrium definition\n",
    "def measure_for_correlated_eq(game, single_run_emp_dist):\n",
    "    erg = [0 for j in range(game.num_steps)]\n",
    "    for i in range(game.num_steps):\n",
    "        vector = is_correlated_eq(game, single_run_emp_dist[i])\n",
    "        # Measure: Maximum Entry        \n",
    "        erg[i] = sum(np.maximum(vector,0))\n",
    "    return erg\n",
    "\n",
    "\n",
    "# ONLY FOR 2 players\n",
    "# Computes for a SINGLE distribution whether that is a correlated equilibrium using the standard definition \n",
    "# the distribution is a correlated equilibrium if all entries of the result vector are <= 0\n",
    "def is_correlated_eq(game, distribution):\n",
    "    resultVector = [0 for i in range(game.num_players) for j in range(game.num_actions_[i]) for k in range(game.num_actions_[i])] \n",
    "    # Entry for every player, for every possible action j and k (for j=k term is 0 trivially) \n",
    "    for i in range(game.num_players):\n",
    "        num_actions_i = game.num_actions_[i]\n",
    "        for j in range(num_actions_i):\n",
    "            for k in range(num_actions_i):\n",
    "                temp_sum = 0\n",
    "                # two actions version, for three player version use replace at index\n",
    "                for h in range(num_actions_i): # actually num_actions of other player\n",
    "                    if i == 0:\n",
    "                        action = tuple([j,h])\n",
    "                        action_index = num_actions_i*j+h\n",
    "                        altern_action = tuple([k,h]) \n",
    "                    elif i == 1:\n",
    "                        action = tuple([h,j])\n",
    "                        action_index = num_actions_i*h+j\n",
    "                        altern_action = tuple([h,k]) \n",
    "                    temp = distribution[action_index]*(game.game_matrix[altern_action][i] - game.game_matrix[action][i]) \n",
    "                    temp_sum = temp_sum + temp\n",
    "                    temp_index = i*num_actions_i*num_actions_i+ num_actions_i*j+k\n",
    "                resultVector[temp_index] = temp_sum\n",
    "    return resultVector\n",
    "\n",
    "\n",
    "# progress bar\n",
    "def update_progress(game, progress, action):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    if(text_output_mode == 0):\n",
    "        clear_output(wait = True)\n",
    "        text1 = game.name +' version '+str(game.version)+ action\n",
    "        text = \"Progress {0}: [{1}] {2:.1f}%\".format(text1, \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "        print(text)\n",
    "        \n",
    "\n",
    "def save_game_data(game):\n",
    "    # Save game                                  \n",
    "    predetermined = game.get_predet_save_text()\n",
    "    filename = 'Data/data_'+game.name+'_version'+ game.version+'_'+str(game.num_runs)+'runs_'+str(game.num_steps)+'steps'+predetermined\n",
    "    print('Saving as '+filename)\n",
    "    file = open(filename,\"wb\")\n",
    "    pickle.dump(game, file)\n",
    "    file.close()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entropy_values(game):\n",
    "    emp_dists = game.emp_dists\n",
    "    [n,m,k] = np.shape(emp_dists)\n",
    "    entropy_values =[]\n",
    "    for run in range(n):\n",
    "        temp_vals = np.zeros(m)\n",
    "        for step in range(m):\n",
    "            dist = emp_dists[run,step,:]\n",
    "            val_entries = [(np.log(dist[i])*dist[i])/np.log(k) if dist[i] > 0  else 0  for i in range(len(dist))]\n",
    "            temp_vals[step] = -sum(val_entries)\n",
    "        entropy_values.append(temp_vals)\n",
    "    game.entropy_values = np.array(entropy_values)\n",
    "    if hasattr(game, 'probs'):\n",
    "        entropy_probs =[]\n",
    "        for run in range(n):\n",
    "            temp_vals = np.zeros(m)\n",
    "            for step in range(m):\n",
    "                dist = (game.probs[run,step,:]).flatten()\n",
    "                val_entries = [(np.log(dist[i])*dist[i])/np.log(k) if dist[i] > 0  else 0  for i in range(len(dist))]\n",
    "                temp_vals[step] = -sum(val_entries)\n",
    "        entropy_probs.append(temp_vals)\n",
    "        game.entropy_probs = np.array(entropy_probs)\n",
    "        \n",
    "     \n",
    "    \n",
    "def simulate_light(game):\n",
    "    start_time_total = time.time()\n",
    "    history_ = []\n",
    "    payoff_diffs_ = []\n",
    "    for j in range(game.num_runs):\n",
    "        [game_history, probs, payoff_diffs] = start_run(game)\n",
    "        history_.append(game_history)\n",
    "        payoff_diffs_.append(payoff_diffs)\n",
    "        if (j*1000/game.num_runs)% 1 == 0:\n",
    "            update_progress(game, j/ game.num_runs,' simulating game')\n",
    "    update_progress(game, 1,' simulating game')\n",
    "    game.save_history(history_) \n",
    "    print('Elapsed: %.2f seconds' % (time.time() - start_time_total))\n",
    "    time.sleep(1)\n",
    "    add_emp_dists(game)\n",
    "    compute_corr_eq_values(game) \n",
    "    add_entropy_values(game)\n",
    "    game.delete_history_data()\n",
    "    print('Total time elapsed: %.2f seconds' % (time.time() - start_time_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress matching_pennies version 2 compute ce-distances: [####################] 100.0%\n",
      "Elapsed: 0.72 seconds\n",
      "Total time elapsed: 5.46 seconds\n",
      "Saving as Data/data_matching_pennies_version2_1runs_10000steps\n"
     ]
    }
   ],
   "source": [
    "# Run the game several times for a fixed number of steps\n",
    "games = ['matching_pennies'] #['matching_pennies', 'chicken', 'prisoners_dilemma', 'bach_stavinsky']\n",
    "num_steps = 10000\n",
    "versions = [\"2\"]  # 1: Base Rule 2: Proxy-Regrets #1': Base rule without inertia\n",
    "# 2':Proxy regrets without inertia 2b: Proxy-Regrets, probabilies as part of both terms \n",
    "# 2c: Proxy-Regrets, probRatio with second sum 2d: Averages \n",
    "num_runs = 1\n",
    "text_output_mode = 0 # 0 = process bar +standard run\n",
    "version_predetermined_steps = 0;\n",
    "for game_name in games:\n",
    "    for version in versions:\n",
    "        temp_game = GameData(game_name, version, num_runs, num_steps)\n",
    "        simulate(temp_game) \n",
    "        save_game_data(temp_game) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
